{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Causal STFT FNet: An Autoregressive Decoder Architecture\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/dataopsnick/FNet-2025/blob/main/Causal_STFT_FNet_Student_Teacher_Knowledge_Distillation.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This notebook implements a novel **Causal Short-Time Fourier Transform (STFT) FNet decoder** that extends the original FNet architecture [1] to autoregressive language modeling. The original FNet paper primarily focused on encoder architectures for tasks like masked language modeling, replacing self-attention with Fourier transforms to achieve significant speedup with competitive performance.\n",
        "\n",
        "## Background: FNet and the Decoder Challenge\n",
        "\n",
        "The original FNet [1] demonstrated that replacing self-attention sublayers with unparameterized Fourier transforms could achieve 92-97% of BERT's accuracy on GLUE benchmarks while training 80% faster on GPUs. However, the paper focused exclusively on encoder architectures, using 2D Discrete Fourier Transforms across both sequence and hidden dimensions.\n",
        "\n",
        "As the authors noted in their conclusion:\n",
        "\n",
        "> \"Throughout this work we have restricted our focus to encoders. FNet decoders can be designed by \"causally\" masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that cross-attention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work...\"\n",
        "\n",
        "## Key Contributions of This Implementation:\n",
        "\n",
        "1. **Causal STFT Layer**: Unlike the original FNet which applies 2D FFT across the entire sequence, this implementation introduces a windowed Short-Time Fourier Transform with causal masking. This enables autoregressive generation while maintaining O(N log N) complexity within each window.\n",
        "\n",
        "2. **Practical Decoder Architecture**: This notebook provides a concrete implementation of a causal FNet decoder for language modeling, addressing the challenge left open by the original paper.\n",
        "\n",
        "3. **Sliding Window FFT Approach**: By using overlapping windows with configurable `stft_window_size` and applying FFT within each window, the model can capture local frequency patterns while maintaining strict causality through careful padding strategies.\n",
        "\n",
        "4. **Knowledge Distillation Framework**: The implementation demonstrates how to train this novel architecture using knowledge distillation from a pre-trained transformer model (Qwen2-0.5B), showing a practical path to competitive performance.\n",
        "\n",
        "## Technical Implementation Details:\n",
        "\n",
        "- **Architecture**: Causal FNet decoder with configurable layers, hidden dimensions, and STFT window sizes\n",
        "- **Causal Mechanism**: Uses zero-padding and strided tensor operations to ensure each position only attends to previous positions\n",
        "- **Training Dataset**: GSM8K mathematical reasoning dataset for demonstrating capabilities on structured reasoning tasks\n",
        "- **Model Sizes**: Demonstration uses a 4-layer, 256-hidden dimension student model for efficient training\n",
        "- **Teacher Model**: Qwen2-0.5B-Instruct for knowledge distillation\n",
        "\n",
        "## Comparison with Official FNet Implementation:\n",
        "\n",
        "The official HuggingFace implementation (`transformers.models.fnet`) provides:\n",
        "- Encoder-only models (FNetModel, FNetForMaskedLM, etc.)\n",
        "- 2D FFT mixing across full sequences\n",
        "- Support for TPU-optimized Fourier transforms\n",
        "\n",
        "This implementation differs by:\n",
        "- Focusing on decoder/autoregressive architectures\n",
        "- Using windowed STFT instead of full-sequence FFT\n",
        "- Implementing causal masking at the architecture level\n",
        "- Demonstrating knowledge distillation for training efficiency\n",
        "\n",
        "## References:\n",
        "\n",
        "[1] J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon, \"FNet: Mixing Tokens with Fourier Transforms,\" arXiv preprint arXiv:2105.03824, 2022.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "29MTNgy2FTVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <h1> Autoregressive FNet with Causal STFT Layer</h1>\n",
        "#@markdown This code cell implements a causally correct and efficient FNet decoder.\n",
        "#@markdown ---\n",
        "#@markdown ### **Code Structure**\n",
        "#@markdown 1.  **Setup**: Installs specific, stable library versions.\n",
        "#@markdown 2.  **Model Definition**: Contains the Causal STFT FNet architecture.\n",
        "#@markdown 3.  **Data Loading & Preprocessing**: Prepares the GSM8K dataset.\n",
        "#@markdown 4.  **Training**: Trains the model using the Hugging Face `Trainer`.\n",
        "#@markdown 5.  **Inference**: Demonstrates text generation with the final, working model.\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 1. Setup\n",
        "#@markdown **IMPORTANT**: Before running, please go to the menu and select \"Runtime -> Restart runtime\".\n",
        "!pip install transformers>=4.31.0 datasets==2.18.0 accelerate>=0.21.0 evaluate==0.4.1 torch peft==0.10.0 -q\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 2. Model Definition\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.fft\n",
        "from torch.nn import CrossEntropyLoss, functional as F\n",
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "from transformers.generation import GenerationMixin\n",
        "from typing import Optional, Tuple, Dict, Any, Union\n",
        "\n",
        "# --- Configuration Class ---\n",
        "class FNetConfig(PretrainedConfig):\n",
        "    model_type = \"causal_stft_fnet\"\n",
        "    def __init__(\n",
        "        self, vocab_size=50257, hidden_size=768, num_hidden_layers=12, intermediate_size=3072,\n",
        "        hidden_dropout_prob=0.1, max_position_embeddings=1024, stft_window_size=64,\n",
        "        initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=50256,\n",
        "        tie_word_embeddings=True, **kwargs,\n",
        "    ):\n",
        "        super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)\n",
        "        self.vocab_size, self.hidden_size, self.num_hidden_layers, self.intermediate_size, \\\n",
        "        self.hidden_dropout_prob, self.max_position_embeddings, self.stft_window_size, \\\n",
        "        self.initializer_range, self.layer_norm_eps = vocab_size, hidden_size, num_hidden_layers, \\\n",
        "        intermediate_size, hidden_dropout_prob, max_position_embeddings, stft_window_size, \\\n",
        "        initializer_range, layer_norm_eps\n",
        "\n",
        "# --- Invented Layer: Causal Short-Time Fourier Transform ---\n",
        "class CausalSTFTLayer(nn.Module):\n",
        "    def __init__(self, config: FNetConfig):\n",
        "        super().__init__()\n",
        "        self.window_size = config.stft_window_size\n",
        "        self.projection = nn.Linear(config.stft_window_size * config.hidden_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        padded_x = F.pad(x, (0, 0, self.window_size - 1, 0))\n",
        "        windows = padded_x.as_strided(\n",
        "            size=(batch_size, seq_len, self.window_size, hidden_size),\n",
        "            stride=(padded_x.stride(0), padded_x.stride(1), padded_x.stride(1), padded_x.stride(2))\n",
        "        )\n",
        "        fft_windows = torch.fft.fftn(windows, dim=(-2, -1)).real\n",
        "        fft_windows = fft_windows.view(batch_size, seq_len, -1)\n",
        "        return self.projection(fft_windows)\n",
        "\n",
        "# --- Standard Layers ---\n",
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, config: FNetConfig):\n",
        "        super().__init__()\n",
        "        self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.activation = nn.GELU()\n",
        "        self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    def forward(self, x): return self.dropout(self.dense2(self.activation(self.dense1(x))))\n",
        "\n",
        "class CausalFNetEncoderBlock(nn.Module):\n",
        "    def __init__(self, config: FNetConfig):\n",
        "        super().__init__()\n",
        "        self.causal_stft = CausalSTFTLayer(config)\n",
        "        self.ffn = FeedForwardLayer(config)\n",
        "        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    def forward(self, x):\n",
        "        x = self.norm1(x + self.causal_stft(x))\n",
        "        x = self.norm2(x + self.ffn(x))\n",
        "        return x\n",
        "\n",
        "class FNetEmbeddings(nn.Module):\n",
        "    def __init__(self, config: FNetConfig):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.pos_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "    def forward(self, input_ids):\n",
        "        seq_len = input_ids.size(1)\n",
        "        pos_ids = self.position_ids[:, :seq_len]\n",
        "        embeds = self.word_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
        "        return self.dropout(self.norm(embeds))\n",
        "\n",
        "# --- Top-Level Causal Model ---\n",
        "class CausalFNetForCausalLM(PreTrainedModel, GenerationMixin):\n",
        "    config_class = FNetConfig\n",
        "    def __init__(self, config: FNetConfig):\n",
        "        super().__init__(config)\n",
        "        self.embeddings = FNetEmbeddings(config)\n",
        "        self.encoder = nn.ModuleList([CausalFNetEncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self): return self.embeddings.word_embeddings\n",
        "    def set_input_embeddings(self, value): self.embeddings.word_embeddings = value\n",
        "    def get_output_embeddings(self): return self.lm_head\n",
        "    def set_output_embeddings(self, new_embeddings): self.lm_head = new_embeddings\n",
        "\n",
        "    def forward(self, input_ids, labels=None, **kwargs):\n",
        "        x = self.embeddings(input_ids)\n",
        "        for block in self.encoder: x = block(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
        "        return CausalLMOutput(loss=loss, logits=logits)\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs: Any) -> Dict[str, Any]:\n",
        "        return {\"input_ids\": input_ids}\n",
        "\n",
        "print(\"‚úÖ Causal STFT FNet model definitions are ready.\")\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 3. Data Loading & Preprocessing\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "DATASET_NAME = \"gsm8k\"\n",
        "TOKENIZER_NAME = \"gpt2\"\n",
        "MODEL_OUTPUT_DIR = \"causal-stft-fnet-gsm8k-finetuned\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "raw_datasets = load_dataset(DATASET_NAME, \"main\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    text = [f\"Question: {q}\\nAnswer: {a}{tokenizer.eos_token}\" for q, a in zip(examples['question'], examples['answer'])]\n",
        "    return tokenizer(text, truncation=True, max_length=256)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
        "train_dataset = tokenized_datasets[\"train\"].select(range(2000))\n",
        "eval_dataset = tokenized_datasets[\"test\"].select(range(200))\n",
        "\n",
        "print(\"‚úÖ Data preprocessing complete.\")\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 4. Training\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "fnet_config = FNetConfig(\n",
        "    vocab_size=tokenizer.vocab_size, pad_token_id=tokenizer.pad_token_id,\n",
        "    hidden_size=256, num_hidden_layers=4, intermediate_size=1024,\n",
        "    max_position_embeddings=1024, stft_window_size=32\n",
        ")\n",
        "model = CausalFNetForCausalLM(fnet_config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=MODEL_OUTPUT_DIR,\n",
        "    num_train_epochs=25,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    save_safetensors=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "print(\"\\n‚úÖ Trainer is configured and ready.\")\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 5. Execute Training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"üéâ Training complete!\")\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ## 6. Inference (Manual Method)\n",
        "# Load the best model\n",
        "best_model_path = trainer.state.best_model_checkpoint\n",
        "print(f\"Loading best model from: {best_model_path}\")\n",
        "\n",
        "model_for_inference = CausalFNetForCausalLM.from_pretrained(best_model_path)\n",
        "model_for_inference.eval()\n",
        "\n",
        "# Move to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_for_inference = model_for_inference.to(device)\n",
        "\n",
        "# Test on a sample question\n",
        "sample_question = raw_datasets[\"test\"][15][\"question\"]\n",
        "prompt = f\"Question: {sample_question}\\nAnswer:\"\n",
        "print(f\"\\nPROMPT:\\n{prompt}\")\n",
        "\n",
        "# Manual generation\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(200):  # Generate up to 100 tokens\n",
        "        outputs = model_for_inference(input_ids=input_ids)\n",
        "        next_token_logits = outputs.logits[0, -1, :]\n",
        "\n",
        "        # Apply temperature (optional)\n",
        "        temperature = 0.8\n",
        "        next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        # Get probabilities and sample\n",
        "        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append to sequence\n",
        "        input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        # Stop if EOS token is generated\n",
        "        if next_token.item() == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "print(\"\\nMODEL GENERATION:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5bec37662c7b4b27938bf692777cd66e",
            "c2bbf386c90c44f3afde3b55d4c92247",
            "c1a4f37d5d64466a889eab1d29c6b752",
            "7ca92e1ad7f240bb9377ad9feb954ceb",
            "519d974dae5a4408b8a2b9fd576f47a5",
            "9d8899413fac4bafac234c8026f0ba13",
            "d883f774a13f49c78050a391362ecec0",
            "f521bccc3b244f45b4be17886077bf75",
            "985f2628117942c38a107e354dc9a363",
            "f32916aafa854e6bbf0d02389159e489",
            "6471b87ce10e449789e4f76059ad6b10",
            "3b3989af3ab44e61b6561987d88e5bdc",
            "5f965940202e43c68a90cb0d6cd64e6d",
            "db883a906eb6419ca3cb5394b00cddf3",
            "f85c74d763ab44dc9be89c178e07140b",
            "0d159a6827d84dd1b30de52f6a3b5df3",
            "0546b127a93a40bf895cbfe4a6efc5da",
            "69639fef767b4c35a6072298a1dbbf1b",
            "8878ca496f2a422ca1a340ab94837470",
            "144145da2e904de7b677018ffaa9ffc2",
            "2d447d6ee71f4a02a73d7e6c2ccfa99c",
            "25c22b94061f4dff854a26b205ef9fa4"
          ]
        },
        "id": "uFuIefcTEyqH",
        "outputId": "f48bef20-727b-41e6-f828-324b4ee165a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Causal STFT FNet model definitions are ready.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5bec37662c7b4b27938bf692777cd66e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b3989af3ab44e61b6561987d88e5bdc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data preprocessing complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-3987084182.py:185: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Trainer is configured and ready.\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 04:07, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>22.669100</td>\n",
              "      <td>21.879351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>16.502600</td>\n",
              "      <td>16.027336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>12.629400</td>\n",
              "      <td>12.380377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>10.294500</td>\n",
              "      <td>10.241842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>8.928600</td>\n",
              "      <td>9.099848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>7.967400</td>\n",
              "      <td>8.461625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>7.294300</td>\n",
              "      <td>8.106636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>6.744300</td>\n",
              "      <td>7.856195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>6.243800</td>\n",
              "      <td>7.735987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.724700</td>\n",
              "      <td>7.659140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>5.417100</td>\n",
              "      <td>7.781919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>5.181100</td>\n",
              "      <td>7.849003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>4.773200</td>\n",
              "      <td>7.875361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>4.493700</td>\n",
              "      <td>8.079125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>4.211500</td>\n",
              "      <td>8.173173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>4.052300</td>\n",
              "      <td>8.323333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.754500</td>\n",
              "      <td>8.707208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.642100</td>\n",
              "      <td>8.867218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.398600</td>\n",
              "      <td>8.978442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.182200</td>\n",
              "      <td>9.171626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.144700</td>\n",
              "      <td>9.392169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>3.011300</td>\n",
              "      <td>9.515717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.934200</td>\n",
              "      <td>9.770499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.816900</td>\n",
              "      <td>9.944734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.710700</td>\n",
              "      <td>10.058707</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Training complete!\n",
            "Loading best model from: causal-stft-fnet-gsm8k-finetuned/checkpoint-2500\n",
            "\n",
            "PROMPT:\n",
            "Question: A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?\n",
            "Answer:\n",
            "\n",
            "MODEL GENERATION:\n",
            "Question: A merchant wants to make a choice of purchase between 2 purchase plans: jewelry worth $5,000 or electronic gadgets worth $8,000. His financial advisor speculates that the jewelry market will go up 2.5% while the electronic gadgets market will rise 1.2% within the same month. If the merchant is looking to maximize profit at the end of this month by making a choice, how much profit would this be?\n",
            "Answer: pool gave 80 the going, theThen, x to305 each Taylor* $15/./10=<<3-6=2>>4\n",
            "00 rest50.\n",
            " It save three is00 water50 quarter Lorenzo 2 2 a<<4*2*6= store>>5.\n",
            ".4 today he he-25*6 = $<<35/120=4>>10>>.\n",
            "####>> the.\n",
            " drank#### humans split was 4+3=<<2+50-8= visitors>>.\n",
            "#### total its.\n",
            "#### W = <<<<15+35=35>> at\n",
            "\n",
            "#### the the825 Camden+ 3 theence on total is + the he = <<25+6=32>>15 years For Caleb\n",
            "In#### has#### 78 = <<50-4=15>>6:1115 the left220\n",
            "#### total The\n",
            " - = 2 of -11#### 5 <<4- << per32=- were>> send23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "02cf22b1e95b4dc1a03817cd785c81a7",
            "41a20e73f44140f78fdd75d6c1422d77",
            "8f4907cb440645a79def47fa4d69e10c",
            "14f70843023c439e9060f825963b1c7b",
            "c8e5d3a8d7d849f4ad0f713ada757f7a",
            "7bfa16bb2b914114983e7c8451561713",
            "114d3101fd944f358b6b4d75e3baa747",
            "acdb6b6d08c2434984343da58e68bb7c",
            "d674a3ef21f94bbebb872a94acc3da3e",
            "48ef8edb6df142d380cf63336fdb2d78",
            "e53c5d7b197c4e728f3f3ed844aac7e7",
            "a9a930cd77784f10bf8a1bb863e4af42",
            "689ed006c4ae4da49fe6df244207bbfb",
            "06d4ff14969543b2b54e69bffb1cee6b",
            "69569c61bf8841298e0a333aa86d2078",
            "b45bbed3169b41a7ac15c18c14b6ffc0",
            "a412aceed8aa445b8847e41c90038346",
            "ade661f0a03048cfaf4d1114c5bc43f4",
            "887ae1204e9546a289fe2e383d119ce0",
            "b1213918a4d14dcdbeab89a176061fdb",
            "2c7f5758f86643b8b99c989fa914b7c7",
            "da5a0fe29d724f4c990730e19c6e1a1b",
            "8c75bba541da43699b0a4636ca947f8d",
            "56c054f339f04ef2ad511838b83f139f",
            "75d9a4ff2cb046e5a49c34e8deed254c",
            "9d13d014eab24ec7a2b8854c6228cec2",
            "6e7d61665dbc462eba2fda5e63f6df84",
            "f73d8eb0c8e94059b2d4d404dcd8a532",
            "aaa3922e875d49fe81bb8f47f8d0fd67",
            "15c05cc1900d4debb9ad56cb60522c5b",
            "e397f65a53f94746822371922b4ed72a",
            "6f9a7186ef5649f086781a71bb97d751",
            "48cf3ab15c984137839e546298898c77",
            "72051a53a1f8415ea32a51f43fcde5e6",
            "3a5b0466a4ed4f1caecf2b1a077d040c",
            "ce2e3e2b6a4547559745811beaf3e95a",
            "faa89a4883234f918e04363616f68277",
            "ea8886b118e04d7da1d8ee1899a8a7d2",
            "168f3f5593214767ae08a1cdbbdc5baf",
            "789a5cd1aee64d4aa958f951cdf49a8b",
            "18ec5ea408e144a98536300b2e445370",
            "9994af736f9d41c3ae0eec7dd956846d",
            "d85b7a969745439ea132baf617b2819b",
            "80e5901f9a51474782a5d5c32c6a9c3a",
            "51dc45b8dbd64966b2561e75bfcf4472",
            "c48bd4a37a3e41a8a492fac8ef493602",
            "1229d983c2c245c2b7aa243b07d235f8",
            "4860ca028fae4485ae9c12cb0cfac32d",
            "47c3f25ee27f471d9170bfe7f5cf558f",
            "b6126bb91f924513b900d8d35f5de268",
            "4336b43011144642867761d487a3c1cb",
            "f7b864fa032c48a9bab1b79373b15b0c",
            "f830ad2cf41a4ffea0b2c652fbea6d61",
            "44d43062953d4a979cce496df5af3192",
            "5471f1492c754193b10ec6c2236c4f50"
          ]
        },
        "id": "cfAuTSrbEhr7",
        "outputId": "2ad4dbe6-1520-4a55-9f69-ab431779a063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model and Distillation Trainer definitions are ready.\n",
            "Loading tokenizer...\n",
            "Fixing out-of-bounds eos_token_id: 151645\n",
            "Vocab size: 151643\n",
            "Pad token: '√¢¬Ωƒπ', Pad token ID: 151642\n",
            "EOS token: '√¢¬Ωƒπ', EOS token ID: 151642\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02cf22b1e95b4dc1a03817cd785c81a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9a930cd77784f10bf8a1bb863e4af42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data preprocessing complete.\n",
            "Sample data:\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72√¢¬Ωƒπ‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó‚Ωó\n",
            "\n",
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "Memory allocated: 0.42 GB\n",
            "Memory available: 39.56 GB\n",
            "\n",
            "Loading teacher model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c75bba541da43699b0a4636ca947f8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72051a53a1f8415ea32a51f43fcde5e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51dc45b8dbd64966b2561e75bfcf4472"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher model vocab size: 151936\n",
            "Tokenizer vocab size: 151643\n",
            "‚úÖ Teacher model loaded successfully\n",
            "\n",
            "Configuring student model with vocab_size=151936, pad_token_id=151642\n",
            "\n",
            "Initializing student model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-2608532673.py:270: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DistillationTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Teacher, Student, and Trainer are configured and ready.\n",
            "Student model parameters: 615,856,128\n",
            "Teacher model parameters: 494,032,768\n",
            "\n",
            "Starting knowledge distillation training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/800 34:24, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>106838.820000</td>\n",
              "      <td>10338.692383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>78271.410000</td>\n",
              "      <td>8651.566406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>67546.100000</td>\n",
              "      <td>8327.429688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>65409.840000</td>\n",
              "      <td>8182.569824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>62704.525000</td>\n",
              "      <td>8095.097656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>61995.010000</td>\n",
              "      <td>8039.615234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>61726.525000</td>\n",
              "      <td>7977.393066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>62867.260000</td>\n",
              "      <td>7943.368652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>60719.510000</td>\n",
              "      <td>7919.916992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>60511.810000</td>\n",
              "      <td>7891.082031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>61906.285000</td>\n",
              "      <td>7874.917480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>59827.310000</td>\n",
              "      <td>7866.100098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>59607.755000</td>\n",
              "      <td>7851.415039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>59369.440000</td>\n",
              "      <td>7839.501953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>60852.790000</td>\n",
              "      <td>7827.914551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>58747.455000</td>\n",
              "      <td>7812.669922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>58763.845000</td>\n",
              "      <td>7805.948242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>60169.525000</td>\n",
              "      <td>7794.398926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>58252.555000</td>\n",
              "      <td>7784.024902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>58052.755000</td>\n",
              "      <td>7778.027344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>58045.025000</td>\n",
              "      <td>7772.632324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>59604.445000</td>\n",
              "      <td>7767.812988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>57749.675000</td>\n",
              "      <td>7763.608887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>57754.185000</td>\n",
              "      <td>7759.255859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>57634.065000</td>\n",
              "      <td>7756.844238</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Distillation training complete!\n",
            "\n",
            "Loading best student model from: distilled-stft-fnet-gsm8k/checkpoint-800\n",
            "\n",
            "PROMPT:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
            "Answer:\n",
            "\n",
            "STUDENT MODEL GENERATION:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\n",
            "Answer:, \"d many toÎïÄ Jenkins clar onitan530 tantal note2 shopper240 per2·•£00350ÔøΩpq Incorrect ComplexityÈ´ò‰ª∑  Ìè¨Ìï® longolvers_finishudas x√©tÂèëÊå•‰∫ÜËûçËµÑ Lust23 Blocked„Éë„Çø„Éº„É≥ mythical Tel\n"
          ]
        }
      ],
      "source": [
        "#@title <h1>Knowledge Distillation: Causal STFT FNet with Qwen2 Teacher (FIXED)</h1>\n",
        "#@markdown This code cell modifies the original Causal STFT FNet to perform knowledge distillation.\n",
        "#@markdown It uses a powerful, pre-trained Qwen/Qwen2-0.5B-Instruct model as the \"teacher\" to train a smaller, more efficient FNet \"student\" model on the GSM8K math reasoning dataset.\n",
        "#@markdown\n",
        "#@markdown ### **Code Structure**\n",
        "#@markdown 1. Setup: Installs required libraries.\n",
        "#@markdown 2. Model Definitions:\n",
        "#@markdown - CausalSTFT FNet: The student model architecture.\n",
        "#@markdown - DistillationTrainer: A custom Hugging Face Trainer to handle the specialized loss calculation.\n",
        "#@markdown 3. Data Loading & Preprocessing: Prepares the GSM8K dataset.\n",
        "#@markdown 4. Training: Initializes the teacher and student models and starts the distillation process.\n",
        "#@markdown 5. Inference: Demonstrates text generation with the trained student model.\n",
        "#@markdown ---\n",
        "#@markdown ## 1. Setup\n",
        "#@markdown IMPORTANT: Before running, select \"Runtime -> Restart runtime\" from the menu.\n",
        "!pip install transformers>=4.38.0 datasets==2.18.0 accelerate>=0.21.0 evaluate==0.4.1 torch peft==0.10.0 sentencepiece -q\n",
        "\n",
        "# Set environment variables for debugging\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For better error messages\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # Better memory management\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## 2. Model & Trainer Definitions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import (PreTrainedModel, PretrainedConfig, Trainer, TrainingArguments,\n",
        "AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling)\n",
        "from transformers.modeling_outputs import CausalLMOutput\n",
        "from transformers.generation import GenerationMixin\n",
        "from datasets import load_dataset\n",
        "from typing import Optional, Dict, Any\n",
        "import gc\n",
        "\n",
        "# Clear any existing CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "#--- Configuration for the Student FNet Model ---\n",
        "\n",
        "class FNetConfig(PretrainedConfig):\n",
        "  model_type = \"causal_stft_fnet\"\n",
        "  def __init__(\n",
        "    self, vocab_size=50257, hidden_size=768, num_hidden_layers=12, intermediate_size=3072,\n",
        "    hidden_dropout_prob=0.1, max_position_embeddings=1024, stft_window_size=64,\n",
        "    initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=50256,\n",
        "    tie_word_embeddings=True, gradient_checkpointing=False, **kwargs,\n",
        "    ):\n",
        "    super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.stft_window_size = stft_window_size\n",
        "    self.initializer_range = initializer_range\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.gradient_checkpointing = gradient_checkpointing\n",
        "\n",
        "#--- Student Model Layers ---\n",
        "class FNetConfig(PretrainedConfig):\n",
        "  model_type = \"causal_stft_fnet\"\n",
        "  def __init__(\n",
        "    self, vocab_size=50257, hidden_size=768, num_hidden_layers=12, intermediate_size=3072,\n",
        "    hidden_dropout_prob=0.1, max_position_embeddings=1024, stft_window_size=64,\n",
        "    initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=50256,\n",
        "    tie_word_embeddings=True, gradient_checkpointing=False, **kwargs,\n",
        "    ):\n",
        "    super().__init__(pad_token_id=pad_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.hidden_dropout_prob = hidden_dropout_prob\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.stft_window_size = stft_window_size\n",
        "    self.initializer_range = initializer_range\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.gradient_checkpointing = gradient_checkpointing\n",
        "\n",
        "#--- Student Model Layers ---\n",
        "\n",
        "class CausalSTFTLayer(nn.Module):\n",
        "  def __init__(self, config: FNetConfig):\n",
        "    super().__init__()\n",
        "    self.window_size = config.stft_window_size\n",
        "    self.projection = nn.Linear(config.stft_window_size * config.hidden_size, config.hidden_size)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      batch_size, seq_len, hidden_size = x.shape\n",
        "      padded_x = F.pad(x, (0, 0, self.window_size - 1, 0))\n",
        "      windows = padded_x.as_strided(\n",
        "          size=(batch_size, seq_len, self.window_size, hidden_size),\n",
        "          stride=(padded_x.stride(0), padded_x.stride(1), padded_x.stride(1), padded_x.stride(2))\n",
        "      )\n",
        "      fft_windows = torch.fft.fftn(windows, dim=(-2, -1)).real\n",
        "      fft_windows = fft_windows.view(batch_size, seq_len, -1)\n",
        "      return self.projection(fft_windows)\n",
        "\n",
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self, config: FNetConfig):\n",
        "    super().__init__()\n",
        "    self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.activation = nn.GELU()\n",
        "    self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "  def forward(self, x):\n",
        "    return self.dropout(self.dense2(self.activation(self.dense1(x))))\n",
        "\n",
        "class CausalFNetEncoderBlock(nn.Module):\n",
        "  def __init__(self, config: FNetConfig):\n",
        "    super().__init__()\n",
        "    self.causal_stft = CausalSTFTLayer(config)\n",
        "    self.ffn = FeedForwardLayer(config)\n",
        "    self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.gradient_checkpointing = False\n",
        "\n",
        "  def forward(self, x):\n",
        "      # Original forward pass logic\n",
        "      stft_output = self.causal_stft(x)\n",
        "      norm1_output = self.norm1(x + stft_output)\n",
        "      ffn_output = self.ffn(norm1_output)\n",
        "      output = self.norm2(norm1_output + ffn_output)\n",
        "\n",
        "      if self.gradient_checkpointing and self.training:\n",
        "          # Apply gradient checkpointing around the main computations\n",
        "          def create_custom_forward(module):\n",
        "              def custom_forward(*inputs):\n",
        "                  return module(*inputs)\n",
        "              return custom_forward\n",
        "\n",
        "          # Checkpointing the STFT and Normalization\n",
        "          stft_output = torch.utils.checkpoint.checkpoint(\n",
        "              create_custom_forward(self.causal_stft), x, use_reentrant=False\n",
        "          )\n",
        "          norm1_output = torch.utils.checkpoint.checkpoint(\n",
        "              create_custom_forward(self.norm1), x + stft_output, use_reentrant=False\n",
        "          )\n",
        "\n",
        "          # Checkpointing the FFN and second Normalization\n",
        "          ffn_output = torch.utils.checkpoint.checkpoint(\n",
        "              create_custom_forward(self.ffn), norm1_output, use_reentrant=False\n",
        "          )\n",
        "          output = torch.utils.checkpoint.checkpoint(\n",
        "              create_custom_forward(self.norm2), norm1_output + ffn_output, use_reentrant=False\n",
        "          )\n",
        "          # Note: This specific checkpointing structure might need adjustment\n",
        "          # based on how dependencies are handled. A simpler approach is to\n",
        "          # checkpoint the entire block's forward pass logic. Let's try that.\n",
        "\n",
        "          # Simpler checkpointing: checkpoint the entire block's computation\n",
        "          # However, the Trainer is designed to checkpoint the blocks themselves.\n",
        "          # The logic in the model's forward pass iterating through self.encoder\n",
        "          # and applying checkpointing there is the standard HF way.\n",
        "          # The issue is likely in how the gradient_checkpointing flag is used here.\n",
        "          # Let's revert this forward pass to the non-checkpointed version\n",
        "          # and rely on the model's forward pass logic to handle checkpointing.\n",
        "\n",
        "          # Reverting to the simpler forward logic, assuming model's forward\n",
        "          # will handle conditional checkpointing per block.\n",
        "          # This block was the source of the NameError. Removing the incorrect logic.\n",
        "          pass # Removed the incorrect checkpointing logic here\n",
        "\n",
        "\n",
        "      # The gradient checkpointing should be handled in the top-level model's forward pass\n",
        "      # by wrapping the call to the block. Let's ensure the top-level model's forward\n",
        "      # pass does this correctly.\n",
        "\n",
        "      # Reverting to the original, non-checkpointed forward pass structure\n",
        "      # and fixing the NameError by removing the incorrect 'block' references.\n",
        "      stft_output = self.causal_stft(x)\n",
        "      norm1_output = self.norm1(x + stft_output)\n",
        "      ffn_output = self.ffn(norm1_output)\n",
        "      output = self.norm2(norm1_output + ffn_output)\n",
        "\n",
        "      return output\n",
        "\n",
        "\n",
        "class FNetEmbeddings(nn.Module):\n",
        "  def __init__(self, config: FNetConfig):\n",
        "    super().__init__()\n",
        "    self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "    self.pos_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "    self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "    self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False)\n",
        "  def forward(self, input_ids):\n",
        "    seq_len = input_ids.size(1)\n",
        "    pos_ids = self.position_ids[:, :seq_len]\n",
        "    embeds = self.word_embeddings(input_ids) + self.pos_embeddings(pos_ids)\n",
        "    return self.dropout(self.norm(embeds))\n",
        "\n",
        "#  --- Top-Level Student Model ---\n",
        "class CausalFNetForCausalLM(PreTrainedModel, GenerationMixin):\n",
        "  config_class = FNetConfig\n",
        "  def __init__(self, config: FNetConfig):\n",
        "    super().__init__(config)\n",
        "    self.embeddings = FNetEmbeddings(config)\n",
        "    self.encoder = nn.ModuleList([CausalFNetEncoderBlock(config) for _ in range(config.num_hidden_layers)])\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "    self.gradient_checkpointing = config.gradient_checkpointing\n",
        "    self.post_init()\n",
        "\n",
        "  def get_input_embeddings(self): return self.embeddings.word_embeddings\n",
        "  def set_input_embeddings(self, value): self.embeddings.word_embeddings = value\n",
        "  def get_output_embeddings(self): return self.lm_head\n",
        "  def set_output_embeddings(self, new_embeddings): self.lm_head = new_embeddings\n",
        "\n",
        "  def forward(self, input_ids, labels=None, **kwargs):\n",
        "    x = self.embeddings(input_ids)\n",
        "\n",
        "    # Corrected forward pass with conditional checkpointing per block\n",
        "    for block in self.encoder:\n",
        "        if self.gradient_checkpointing and self.training:\n",
        "            # Apply gradient checkpointing to the *block's forward pass*\n",
        "            x = torch.utils.checkpoint.checkpoint(block, x, use_reentrant=False)\n",
        "        else:\n",
        "            x = block(x) # Call the block's forward pass directly\n",
        "\n",
        "    logits = self.lm_head(x)\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      shift_logits = logits[..., :-1, :].contiguous()\n",
        "      shift_labels = labels[..., 1:].contiguous()\n",
        "      loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
        "    return CausalLMOutput(loss=loss, logits=logits)\n",
        "\n",
        "  # Add method to enable gradient checkpointing\n",
        "  def _set_gradient_checkpointing(self, module, value=False):\n",
        "      if isinstance(module, CausalFNetEncoderBlock):\n",
        "          module.gradient_checkpointing = value\n",
        "      # Recursively apply to children\n",
        "      for child in module.children():\n",
        "          self._set_gradient_checkpointing(child, value)\n",
        "\n",
        "  # Trainer expects this method name\n",
        "  def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "      self._set_gradient_checkpointing(self, True)\n",
        "\n",
        "  def gradient_checkpointing_disable(self):\n",
        "      self._set_gradient_checkpointing(self, False)\n",
        "\n",
        "  # Add method to enable gradient checkpointing\n",
        "  def _set_gradient_checkpointing(self, module, value=False):\n",
        "      if isinstance(module, CausalFNetEncoderBlock):\n",
        "          module.gradient_checkpointing = value\n",
        "      # Recursively apply to children\n",
        "      for child in module.children():\n",
        "          self._set_gradient_checkpointing(child, value)\n",
        "\n",
        "  # Trainer expects this method name\n",
        "  def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "      self._set_gradient_checkpointing(self, True)\n",
        "\n",
        "  def gradient_checkpointing_disable(self):\n",
        "      self._set_gradient_checkpointing(self, False)\n",
        "\n",
        "  def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, **kwargs: Any) -> Dict[str, Any]:\n",
        "    return {\"input_ids\": input_ids}\n",
        "\n",
        "#--- Custom Trainer for Knowledge Distillation ---\n",
        "class DistillationTrainer(Trainer):\n",
        "  def __init__(self, *args, teacher_model=None, alpha=0.5, temperature=2.0, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.teacher_model = teacher_model\n",
        "    self.alpha = alpha\n",
        "    self.temperature = temperature\n",
        "    if self.teacher_model:\n",
        "        self.teacher_model.eval() # Ensure teacher is in eval mode\n",
        "\n",
        "  def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "    # 1. Standard cross-entropy loss from student\n",
        "    student_outputs = model(**inputs)\n",
        "    student_loss = student_outputs.loss if student_outputs.loss is not None else torch.tensor(0.0)\n",
        "\n",
        "    # 2. Distillation loss (KL Divergence)\n",
        "    if self.teacher_model is not None:\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = self.teacher_model(**inputs)\n",
        "\n",
        "        # Get logits, ensuring they are aligned\n",
        "        student_logits = student_outputs.logits\n",
        "        teacher_logits = teacher_outputs.logits\n",
        "\n",
        "        # Shift for next-token prediction\n",
        "        shift_student_logits = student_logits[..., :-1, :].contiguous()\n",
        "        shift_teacher_logits = teacher_logits[..., :-1, :].contiguous()\n",
        "\n",
        "        # Soften probabilities with temperature\n",
        "        soft_student_log_probs = F.log_softmax(shift_student_logits / self.temperature, dim=-1)\n",
        "        soft_teacher_probs = F.softmax(shift_teacher_logits / self.temperature, dim=-1)\n",
        "\n",
        "        # Calculate KL Divergence loss\n",
        "        distillation_loss = F.kl_div(\n",
        "            soft_student_log_probs,\n",
        "            soft_teacher_probs,\n",
        "            reduction='batchmean',\n",
        "            log_target=False\n",
        "        ) * (self.temperature ** 2)\n",
        "\n",
        "        # 3. Combine losses\n",
        "        loss = self.alpha * student_loss + (1.0 - self.alpha) * distillation_loss\n",
        "    else:\n",
        "        loss = student_loss\n",
        "\n",
        "    return (loss, student_outputs) if return_outputs else loss\n",
        "\n",
        "print(\"‚úÖ Model and Distillation Trainer definitions are ready.\")\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## 3. Data Loading & Preprocessing\n",
        "TEACHER_MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "DATASET_NAME = \"gsm8k\"\n",
        "STUDENT_MODEL_OUTPUT_DIR = \"distilled-stft-fnet-gsm8k\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# Fix the Qwen2 tokenizer configuration\n",
        "original_vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# Check if eos_token_id is out of bounds and fix it\n",
        "if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None and tokenizer.eos_token_id >= original_vocab_size:\n",
        "    print(f\"Fixing out-of-bounds eos_token_id: {tokenizer.eos_token_id}\")\n",
        "    # Set eos_token_id to the last valid token\n",
        "    tokenizer.eos_token_id = original_vocab_size - 1\n",
        "    tokenizer.eos_token = tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id)\n",
        "\n",
        "# Now handle pad_token\n",
        "if tokenizer.pad_token is None:\n",
        "    # Use a valid token ID within the vocabulary\n",
        "    if tokenizer.eos_token_id is not None and tokenizer.eos_token_id < original_vocab_size:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        print(f\"Using eos_token as pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "    else:\n",
        "        # Use the last valid token as pad token\n",
        "        tokenizer.pad_token_id = original_vocab_size - 1\n",
        "        tokenizer.pad_token = tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)\n",
        "        print(f\"Using last token as pad_token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "\n",
        "# Ensure pad_token_id is valid\n",
        "if tokenizer.pad_token_id >= original_vocab_size:\n",
        "     tokenizer.pad_token_id = original_vocab_size - 1\n",
        "     tokenizer.pad_token = tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)\n",
        "\n",
        "final_vocab_size = original_vocab_size  # We're not adding new tokens\n",
        "print(f\"Vocab size: {final_vocab_size}\")\n",
        "print(f\"Pad token: '{tokenizer.pad_token}', Pad token ID: {tokenizer.pad_token_id}\")\n",
        "print(f\"EOS token: '{tokenizer.eos_token}', EOS token ID: {tokenizer.eos_token_id}\")\n",
        "\n",
        "# Validate that pad_token_id is within bounds - should now pass\n",
        "assert tokenizer.pad_token_id < final_vocab_size, f\"pad_token_id ({tokenizer.pad_token_id}) must be < vocab_size ({final_vocab_size})\"\n",
        "\n",
        "print(\"Loading dataset...\")\n",
        "raw_datasets = load_dataset(DATASET_NAME, \"main\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  text = [f\"Question: {q}\\nAnswer: {a}{tokenizer.eos_token}\" for q, a in zip(examples['question'], examples['answer'])]\n",
        "  return tokenizer(text, truncation=True, max_length=128, padding='max_length')  # Reduced max_length\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names)\n",
        "train_dataset = tokenized_datasets[\"train\"].select(range(500)) # Using smaller subset\n",
        "eval_dataset = tokenized_datasets[\"test\"].select(range(50))\n",
        "\n",
        "print(\"‚úÖ Data preprocessing complete.\")\n",
        "print(f\"Sample data:\\n{tokenizer.decode(train_dataset['input_ids'][0])}\")\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## 4. Training Setup\n",
        "# Clear GPU cache before starting\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")\n",
        "    print(f\"Memory available: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB\")\n",
        "\n",
        "# --- Load Teacher Model with memory optimization ---\n",
        "print(\"\\nLoading teacher model...\")\n",
        "try:\n",
        "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
        "        TEACHER_MODEL_NAME,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"cuda:0\",\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "\n",
        "    teacher_model.eval()\n",
        "    teacher_model.gradient_checkpointing_enable()\n",
        "\n",
        "    # Get the actual teacher model vocab size\n",
        "    teacher_vocab_size = teacher_model.config.vocab_size\n",
        "    print(f\"Teacher model vocab size: {teacher_vocab_size}\")\n",
        "    print(f\"Tokenizer vocab size: {final_vocab_size}\")\n",
        "\n",
        "    # Use the teacher's vocab size for the student to ensure compatibility\n",
        "    actual_vocab_size = teacher_vocab_size\n",
        "    print(\"‚úÖ Teacher model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading teacher model: {e}\")\n",
        "    print(\"Continuing without teacher model (student-only training)\")\n",
        "    teacher_model = None\n",
        "    actual_vocab_size = final_vocab_size\n",
        "\n",
        "# Ensure pad_token_id is valid for the actual vocab size\n",
        "if tokenizer.pad_token_id >= actual_vocab_size:\n",
        "    tokenizer.pad_token_id = actual_vocab_size - 1\n",
        "    tokenizer.pad_token = tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)\n",
        "    print(f\"Adjusted pad_token_id to {tokenizer.pad_token_id} for vocab size {actual_vocab_size}\")\n",
        "\n",
        "#--- Configure and Initialize Student Model ---\n",
        "student_config = FNetConfig(\n",
        "  vocab_size=actual_vocab_size,  # Use teacher's vocab size\n",
        "  pad_token_id=tokenizer.pad_token_id,  # Use the validated pad_token_id\n",
        "  hidden_size=512,      # Very small model\n",
        "  num_hidden_layers=4,  # Minimal layers\n",
        "  intermediate_size=256,\n",
        "  max_position_embeddings=256,\n",
        "  stft_window_size=512,  # Small window\n",
        "  gradient_checkpointing=True,  # Enable gradient checkpointing\n",
        "  tie_word_embeddings=True  # Tie embeddings to save memory\n",
        ")\n",
        "\n",
        "print(f\"\\nConfiguring student model with vocab_size={actual_vocab_size}, pad_token_id={tokenizer.pad_token_id}\")\n",
        "\n",
        "# Ensure pad_token_id is valid for the actual vocab size\n",
        "if tokenizer.pad_token_id >= actual_vocab_size:\n",
        "    tokenizer.pad_token_id = actual_vocab_size - 1\n",
        "    tokenizer.pad_token = tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id)\n",
        "    print(f\"Adjusted pad_token_id to {tokenizer.pad_token_id} for vocab size {actual_vocab_size}\")\n",
        "\n",
        "print(\"\\nInitializing student model...\")\n",
        "student_model = CausalFNetForCausalLM(student_config)\n",
        "student_model = student_model.to(device)\n",
        "\n",
        "#--- Training Arguments ---\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=STUDENT_MODEL_OUTPUT_DIR,\n",
        "  num_train_epochs=25,  # Fewer epochs\n",
        "  per_device_train_batch_size=2,  # Very small batch size\n",
        "  per_device_eval_batch_size=2,\n",
        "  gradient_accumulation_steps=8,  # Larger accumulation\n",
        "  learning_rate=5e-6,\n",
        "  weight_decay=0.01,\n",
        "  warmup_steps=50,\n",
        "  eval_strategy=\"epoch\",\n",
        "  save_strategy=\"epoch\",\n",
        "  load_best_model_at_end=True,\n",
        "  logging_steps=25,\n",
        "  fp16=True,  # Use mixed precision\n",
        "  gradient_checkpointing=True,\n",
        "  report_to=\"none\",\n",
        "  save_total_limit=1,\n",
        "  dataloader_pin_memory=False,  # Reduce memory usage\n",
        "  dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
        "  save_safetensors=False,  # For tied embeddings\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "#--- Initialize the Custom Trainer ---\n",
        "distillation_trainer = DistillationTrainer(\n",
        "  model=student_model,\n",
        "  teacher_model=teacher_model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=eval_dataset,\n",
        "  tokenizer=tokenizer,\n",
        "  data_collator=data_collator,\n",
        "  alpha=0.5 if teacher_model else 1.0,  # Only use student loss if no teacher\n",
        "  temperature=3.0\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Teacher, Student, and Trainer are configured and ready.\")\n",
        "print(f\"Student model parameters: {sum(p.numel() for p in student_model.parameters()):,}\")\n",
        "if teacher_model:\n",
        "    print(f\"Teacher model parameters: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
        "\n",
        "# Clear cache again before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## 5. Execute Training\n",
        "print(\"\\nStarting knowledge distillation training...\")\n",
        "try:\n",
        "    distillation_trainer.train()\n",
        "    print(\"üéâ Distillation training complete!\")\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    if \"out of memory\" in str(e).lower():\n",
        "        print(\"\\n‚ö†Ô∏è GPU out of memory. Suggestions:\")\n",
        "        print(\"- Restart runtime and try again\")\n",
        "        print(\"- Use gradient accumulation with smaller batch size\")\n",
        "        print(\"- Reduce model size further\")\n",
        "        print(\"- Use CPU training (slower but more stable)\")\n",
        "    raise e\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## 6. Inference with the Distilled Student Model\n",
        "\n",
        "# Clear cache before inference\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "#Load the best student model checkpoint\n",
        "if hasattr(distillation_trainer.state, 'best_model_checkpoint') and distillation_trainer.state.best_model_checkpoint:\n",
        "    best_student_path = distillation_trainer.state.best_model_checkpoint\n",
        "    print(f\"\\nLoading best student model from: {best_student_path}\")\n",
        "    inference_model = CausalFNetForCausalLM.from_pretrained(best_student_path).to(device)\n",
        "else:\n",
        "    print(\"\\nUsing current model state for inference\")\n",
        "    inference_model = student_model\n",
        "\n",
        "inference_model.eval()\n",
        "\n",
        "# Test on a sample question from the test set\n",
        "sample_question = raw_datasets[\"test\"][1][\"question\"]\n",
        "prompt = f\"Question: {sample_question}\\nAnswer:\"\n",
        "print(f\"\\nPROMPT:\\n{prompt}\")\n",
        "\n",
        "#Generate an answer using the distilled student model\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    output_sequences = inference_model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=50,  # Reduced for memory\n",
        "      num_return_sequences=1,\n",
        "      do_sample=True,\n",
        "      top_k=50,\n",
        "      top_p=0.95,\n",
        "      temperature=0.7,\n",
        "      pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(\"\\nSTUDENT MODEL GENERATION:\")\n",
        "print(generated_text)\n",
        "\n",
        "# Final cleanup\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    }
  ]
}