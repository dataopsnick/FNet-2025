\documentclass[11pt,a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{url}
\usepackage{enumitem}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Causal FNet: An Autoregressive Decoder via Short-Time Fourier Transform}
\author{ASI Research Lab\\
        CyberGolem LLC\\
        \texttt{asi@cybergolem.ai}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
The FNet architecture demonstrated that replacing self-attention with unparameterized Fourier transforms in an encoder can achieve competitive performance to BERT with significant training speedups. However, the original work focused exclusively on encoders, leaving the design of a causal FNet decoder for autoregressive tasks as an open question. This paper addresses this gap by introducing the Causal Short-Time Fourier Transform (STFT) FNet, a novel decoder-only architecture. Our core contribution is the Causal STFT layer, which applies a Fourier transform over a sliding, causally-masked window of tokens, preserving the autoregressive property essential for generation while maintaining O(N log N) complexity within the window. We present a complete implementation of this architecture and evaluate its viability through two training paradigms on the GSM8K mathematical reasoning dataset. First, we train a small-scale model from scratch, demonstrating that the architecture is capable of learning. Second, we employ knowledge distillation, using a pre-trained Qwen2-0.5B-Instruct model as a teacher to train a Causal STFT FNet student. Our results show that while training from scratch is feasible, knowledge distillation provides a more stable and effective training signal, achieving a significantly lower validation loss. This work provides a concrete and practical design for an FNet-style decoder, fulfilling the challenge posed by the original authors and offering a promising new direction for efficient, attention-free generative models.
\end{abstract}

\section{Introduction}
The Transformer architecture \citep{vaswani2017attention} has become the foundation for modern natural language processing, largely due to its self-attention mechanism. However, the quadratic complexity of self-attention with respect to sequence length has spurred research into more efficient alternatives. A notable contribution in this area is FNet \citep{lee2021fnet}, which replaced the self-attention sublayer in a BERT-style encoder with a simple, unparameterized 2D Fourier transform across the sequence and hidden dimensions. The authors showed that FNet could achieve 92-97\% of BERT's accuracy on the GLUE benchmark while training 80\% faster on GPUs.

Despite this success with encoders, the FNet paper explicitly left the extension to decoders as an open problem for future work, stating:

\begin{quote}
    \textit{"Throughout this work we have restricted our focus to encoders. FNet decoders can be designed by `causally' masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question... We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work..."} --- \citet{lee2021fnet}
\end{quote}

This paper directly addresses that challenge. We propose and implement a novel \textbf{Causal Short-Time Fourier Transform (STFT) FNet}, a decoder-only architecture designed for autoregressive language modeling. Our primary innovation is the `CausalSTFTLayer`, which circumvents the need for modifying the FFT algorithm itself. Instead, it uses a sliding window approach with careful padding and tensor striding to ensure that each token's representation is mixed only with those of previous tokens, thereby strictly enforcing causality at the architectural level.

To demonstrate the viability of this new architecture, we conduct two experiments on the GSM8K mathematical reasoning dataset. First, we train a small-scale Causal STFT FNet from scratch to establish a performance baseline. Second, recognizing the immense cost of pre-training large models, we explore a more practical training regimen: \textbf{knowledge distillation}. We use a pre-trained, powerful teacher model (Qwen2-0.5B-Instruct) to train our FNet-based student, leveraging a custom trainer that combines standard cross-entropy loss with a Kullback-Leibler (KL) divergence loss on the models' probability distributions.

Our contributions are:
\begin{enumerate}
    \item A novel \textbf{Causal STFT Layer} that enables the use of Fourier transforms for token mixing in a causally correct, autoregressive manner.
    \item A complete, practical implementation of a \textbf{decoder-only FNet architecture} for language modeling.
    \item A demonstration of training this architecture both \textbf{from scratch} and through \textbf{knowledge distillation}, providing a proof-of-concept for its learning capability and a practical path to achieving competitive performance.
\end{enumerate}

\section{Methodology}
Our goal is to create a causal, decoder-only model that adheres to the principles of FNet by using Fourier transforms for token mixing. The architecture consists of an embedding layer, a stack of identical decoder blocks, and a final language modeling head.

\subsection{Causal FNet Decoder Block}
Each block in our Causal FNet decoder follows the standard pre-norm residual structure. It contains two main sublayers: a token-mixing layer and a position-wise feed-forward network (FFN).
\begin{align}
    x' &= \text{LayerNorm}(x + \text{CausalSTFTLayer}(x)) \\
    x_{out} &= \text{LayerNorm}(x' + \text{FFN}(x'))
\end{align}
The FFN is a standard two-layer network with a GELU activation function. The key innovation lies in the `CausalSTFTLayer`.

\subsection{The Causal Short-Time Fourier Transform (STFT) Layer}
The primary challenge in creating a generative FNet is ensuring causalityâ€”that the prediction for a token at position $t$ depends only on the known tokens at positions $<t$. The original FNet applied a 2D FFT across the entire sequence, which is inherently non-causal as it allows every token to see every other token.

Our \texttt{CausalSTFTLayer} solves this by operating on sliding windows. For an input tensor $x$ of shape \texttt{(batch\_size, seq\_len, hidden\_size)} and a given \texttt{window\_size}, the process is as follows:
\begin{enumerate}
    \item \textbf{Causal Padding}: The input sequence $x$ is padded on the left (the past) with \texttt{window\_size - 1} zero vectors. This ensures that the first token's window contains only itself and padding, and every subsequent token's window contains only itself and preceding tokens.
    \item \textbf{Windowing via Striding}: We use efficient tensor striding (e.g., \texttt{torch.as\_strided}) to create a causally-correct view of the padded tensor with shape \texttt{(batch\_size, seq\_len, window\_size, hidden\_size)}. This avoids memory duplication by creating overlapping windows of \texttt{window\_size} tokens for each position in the sequence.
    \item \textbf{Fourier Transform}: A 2D Fast Fourier Transform (\texttt{torch.fft.fftn}) is applied to each window across the window and hidden dimensions (the last two dimensions). We take the real part of the complex result.
    \item \textbf{Projection}: The transformed windows, now of shape \texttt{(batch\_size, seq\_len, window\_size, hidden\_size)}, are flattened and projected back to the original \texttt{hidden\_size} with a linear layer.
\end{enumerate}
This design effectively mixes information from the local, causal neighborhood of each token in the frequency domain, maintaining the spirit of FNet while respecting the autoregressive constraint.

\subsection{Training via Knowledge Distillation}
While training from scratch is possible, we hypothesize that distilling knowledge from a pre-trained attention-based model is a more effective method for training our novel architecture. We define a student model, $S$ (our Causal STFT FNet), and a larger, pre-trained teacher model, $T$ (Qwen2-0.5B-Instruct).

The training objective is to minimize a composite loss function that combines a standard cross-entropy (CE) loss with a distillation loss. The distillation loss, based on KL-divergence, encourages the student's output distribution to match the teacher's softened output distribution.

The total loss $L_{KD}$ is a weighted sum of the two losses:
\begin{equation}
    L_{KD} = \alpha \cdot L_{CE}(y, \sigma(z_S)) + (1 - \alpha) \cdot L_{Distill}(z_S, z_T)
\end{equation}
where:
\begin{itemize}
    \item $y$ are the true labels (next token).
    \item $z_S$ and $z_T$ are the logits produced by the student and teacher models, respectively.
    \item $\sigma$ is the softmax function.
    \item $\alpha$ is a hyperparameter balancing the two loss terms.
    \item $L_{CE}$ is the standard cross-entropy loss.
    \item $L_{Distill}$ is the KL-divergence loss, calculated with a temperature scaling parameter $\tau$:
    \begin{equation}
        L_{Distill}(z_S, z_T) = D_{KL}(\sigma(z_S/\tau) \Vert \sigma(z_T/\tau)) \cdot \tau^2
    \end{equation}
\end{itemize}
The temperature $\tau$ softens the probability distributions, forcing the student to learn from the nuanced, inter-class relationships present in the teacher's logits.

\section{Experiments and Results}

We conducted two experiments using the GSM8K dataset, which consists of grade-school math word problems. All experiments were performed within a single Jupyter Notebook environment.

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Dataset}: GSM8K (`main` configuration). Text was formatted as `"Question: {q}\\nAnswer: {a}"`.
    \item \textbf{Tokenizer}: GPT2 for the from-scratch experiment; Qwen2 tokenizer for the distillation experiment.
    \item \textbf{Hardware}: A100 GPU on Google Colab.
\end{itemize}

\subsection{Experiment 1: Training from Scratch}
In this experiment, we trained a small Causal STFT FNet to establish a baseline for its learning ability.
\begin{itemize}
    \item \textbf{Model Configuration}: 4 layers, 256 hidden dimensions, 1024 intermediate size, STFT window size of 32.
    \item \textbf{Training}: 25 epochs with a batch size of 8 and a learning rate of 1e-4.
\end{itemize}

\textbf{Results}: The model trained successfully, with the validation loss steadily decreasing from an initial 21.88 to a final 10.06 (best was 7.66 at epoch 10), as shown in Table \ref{tab:loss_results}. This confirms that the architecture is capable of learning patterns from the data. However, as expected for a small model trained from scratch on a specialized dataset, the generated output was largely incoherent.

\begin{table}[h]
\centering
\caption{Validation loss across epochs for both experiments.}
\label{tab:loss_results}
\begin{tabular}{ccc}
\toprule
\textbf{Epoch} & \textbf{Validation Loss (From Scratch)} & \textbf{Validation Loss (Distillation)} \\
\midrule
1  & 21.88 & 10338.69 \\
5  & 9.10  & 8095.10  \\
10 & \textbf{7.66}  & 7891.08  \\
15 & 8.17  & 7827.91  \\
20 & 9.17  & 7778.03  \\
25 & 10.06 & \textbf{7756.84}  \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Sample Generation (From Scratch):}
\begin{verbatim}
PROMPT:
Question: A merchant wants to make a choice of purchase between 
2 purchase plans: jewelry worth $5,000 or electronic gadgets 
worth $8,000... how much profit would this be?
Answer:

MODEL GENERATION:
...pool gave 80 the going, theThen, x to305 each Taylor* 
$15/./10=<<3-6=2>>4
00 rest50.
\end{verbatim}

\subsection{Experiment 2: Knowledge Distillation with Qwen2}
Here, we trained a larger Causal STFT FNet student model by distilling knowledge from the pre-trained Qwen2-0.5B-Instruct teacher.
\begin{itemize}
    \item \textbf{Teacher Model}: `Qwen/Qwen2-0.5B-Instruct`.
    \item \textbf{Student Configuration}: 4 layers, 512 hidden dimensions, 256 intermediate size, STFT window size of 512.
    \item \textbf{Training}: 25 epochs with a batch size of 2, gradient accumulation of 8, learning rate of 5e-6, distillation alpha of 0.5, and temperature of 3.0.
\end{itemize}

\textbf{Results}: The distillation training was also successful and stable. The validation loss consistently decreased throughout training, reaching a final best value of 7756.84 (Table \ref{tab:loss_results}). The absolute loss values are not directly comparable to Experiment 1 due to different tokenizers, model sizes, and loss functions (the distillation loss component is large). However, the stable decrease demonstrates that the student was effectively learning from the teacher's distributions. The generated text, while still not mathematically correct, showed more structural coherence than the from-scratch model.

\textbf{Sample Generation (Distilled Student):}
\begin{verbatim}
PROMPT:
Question: A robe takes 2 bolts of blue fiber and half that 
much white fiber. How many bolts in total does it take?
Answer:

STUDENT MODEL GENERATION:
... "d many toë•€ Jenkins clar onitan530 tantal note2 
shopper240 per2á¥£00350ï¿½pq Incorrect Complexity...
\end{verbatim}

\subsection{Experiment 3: Hyperparameter Ablation Study}
To better understand the architectural trade-offs and identify a more optimal configuration, we conducted a systematic ablation study. We explored the impact of four key hyperparameters: the hidden dimension size (\texttt{d\_model}), the number of layers, the feed-forward network (FFN) intermediate size, and the STFT window size.

\textbf{Setup}: We trained 16 different model configurations, varying the aforementioned hyperparameters. All models were trained using the knowledge distillation setup from Experiment 2 on the GSM8K dataset.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{ablation_plot.png} % Note: Replace with your actual figure file
\caption{Final validation loss vs. number of parameters for all ablation models. Marker size is proportional to training time. The best-performing model is annotated, demonstrating that a higher parameter count does not guarantee a lower loss.}
\label{fig:ablation_plot}
\end{figure}

\textbf{Results Analysis}: The results, detailed in Table~\ref{tab:ablation_results} 
and visualized in Figure~\ref{fig:ablation_plot}, reveal several key insights. 
While performance generally improves with scale, the relationship is not linear. 
The best-performing configuration was \texttt{512\_3\_2048\_32} 
(\texttt{d\_model} = 512, layers = 3, FFN = 2048, window = 32), which 
achieved a validation loss of 36.95. Notably, this model contains only 57.5M 
parameters, significantly outperforming larger models. This suggests that a wider 
FFN combined with a moderately sized STFT window is more parameter-efficient than 
simply increasing layer depth or hidden size indiscriminately. Models with a 
hidden dimension of 512 consistently outperformed smaller variants, establishing it as a critical factor for performance.

\begin{table}[h]
\centering
\caption{Ablation study results, sorted by final validation loss.}
\label{tab:ablation_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Model Config (`d\_model`\_`layers`\_`ffn`\_`window`)} & \textbf{Params (M)} & \textbf{Final Loss} & \textbf{Training Time (min)} \\
\midrule
\textbf{512\_3\_2048\_32} & \textbf{57.47} & \textbf{36.95} & \textbf{4.56} \\
512\_3\_2048\_64 & 82.63 & 39.30 & 6.60 \\
512\_3\_1024\_64 & 79.48 & 39.30 & 6.55 \\
512\_3\_1024\_256 & 230.48 & 39.33 & 19.36 \\
num\_hidden\_layers\_3 & 233.63 & 39.47 & 19.43 \\
512\_3\_2048\_128 & 132.96 & 39.89 & 10.90 \\
512\_3\_1024\_128 & 129.82 & 40.00 & 10.62 \\
256\_3\_1024\_256 & 64.91 & 42.43 & 9.69 \\
256\_3\_1024\_64 & 27.16 & 42.71 & 3.91 \\
256\_3\_1024\_128 & 39.74 & 42.83 & 5.82 \\
128\_3\_1024\_256 & 19.87 & 46.65 & 5.56 \\
128\_3\_1024\_128 & 13.58 & 47.29 & 3.50 \\
128\_3\_1024\_64 & 10.44 & 47.57 & 2.76 \\
128\_6\_512\_64 & 13.58 & 52.43 & 3.87 \\
128\_8\_512\_64 & 15.95 & 52.81 & 4.61 \\
128\_10\_512\_64 & 18.31 & 52.83 & 5.36 \\
\bottomrule
\end{tabular}%
}
\end{table}


\section{Discussion}
The primary goal of this work was to design a viable FNet decoder architecture and demonstrate that it can be trained effectively. Our experiments confirm several key points.

First, the Causal STFT FNet architecture is fundamentally sound. The model trained from scratch (Experiment 1) successfully learned to decrease its validation loss, indicating that the Causal STFT layer and feed-forward networks are sufficient for modeling relationships in sequential data, albeit to a limited degree without large-scale pre-training.

Second, knowledge distillation is a highly effective and practical method for training such a novel architecture (Experiment 2). By leveraging the soft probability distributions from a powerful teacher model, the student FNet received a much richer training signal than what is available from one-hot labels alone. This led to a more stable training process and a model that, while not perfect, exhibits more structure in its generations.

It is crucial to note that the quality of the generated text from both experiments is not state-of-the-art. This is an expected outcome. Achieving high-fidelity text generation requires extensive pre-training on vast and diverse text corpora, which was beyond the scope of this architectural proof-of-concept. Our contribution is not a new state-of-the-art generative model, but rather a novel, efficient, attention-free architecture and a practical methodology for training it.

This implementation differs from the original FNet in several critical ways:
\begin{itemize}
    \item \textbf{Architecture Focus}: We built a decoder-only model for autoregressive generation, whereas the original FNet was an encoder for NLU tasks.
    \item \textbf{Causality Mechanism}: We introduce the STFT over a sliding causal window, a significant departure from the full-sequence 2D FFT used in the original.
    \item \textbf{Training Method}: We highlight knowledge distillation as a primary training method, which is more practical for novel architectures than pre-training from scratch.
\end{itemize}


\section{Conclusion}
In this paper, we have successfully addressed the open challenge of creating a causal FNet decoder for autoregressive text generation. We introduced the Causal STFT FNet, a novel architecture whose core componentâ€”the Causal STFT layerâ€”enables token mixing with Fourier transforms while strictly maintaining causality. Through experiments on the GSM8K dataset, we have shown that this architecture is capable of learning and can be effectively trained using knowledge distillation from a larger, pre-trained teacher model. This work provides a concrete blueprint for future research into efficient, attention-free generative models and opens the door to the possibility of pre-training a large-scale Causal STFT FNet.

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Lee-Thorp et al.(2021)]{lee2021fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon.
\newblock FNet: Mixing Tokens with Fourier Transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Vaswani et al.(2017)]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 5998--6008, 2017.

\end{thebibliography}

\end{document}